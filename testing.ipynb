{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import yaml\n",
    "import datetime as dt\n",
    "from sqlalchemy import create_engine\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_snow = pd.read_csv('swiss_snow.csv')\n",
    "df_swiss_sunshine = pd.read_csv('swiss_sunshine.csv')\n",
    "df_swiss_precipitation = pd.read_csv('swiss_precipitation.csv')\n",
    "df_swiss_temp = pd.read_csv('swiss_temp.csv')\n",
    "df_temp = pd.read_csv('temp.csv')\n",
    "df_co2 = pd.read_csv('co2.csv')\n",
    "\n",
    "countries = pd.read_json('data_collection/countries.json').T\n",
    "\n",
    "credentials = yaml.load(open('config.yml'), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_name(df):\n",
    "    '''\n",
    "    Taken from https://stackoverflow.com/a/50620134/12183550\n",
    "    '''\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "\n",
    "def get_db_table_length(table, credentials):\n",
    "    '''\n",
    "    Get table length form postgres sql db.\n",
    "    \n",
    "    Takes:\n",
    "    - table: table name\n",
    "    - credentials: loaded yaml file\n",
    "    \n",
    "    Returns:\n",
    "    - len_table: length of table\n",
    "    '''\n",
    "    # set up connection parameters\n",
    "    db_name = credentials['sql']['db']\n",
    "    db_user = credentials['sql']['user']\n",
    "    db_user_pw = credentials['sql']['pw']\n",
    "    db_adress = credentials['sql']['host']\n",
    "    db_port = credentials['sql']['port']\n",
    "\n",
    "    # set up connection\n",
    "    conn = psycopg2.connect(database = db_name, user = db_user, password = db_user_pw, host = db_adress, port = db_port)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM {table_name}\".format(table_name = table))\n",
    "    len_table = cursor.fetchone()\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    return len_table[0]\n",
    "\n",
    "def insert_df_into_db_table(df, table, engine):\n",
    "    '''\n",
    "    Takes given Dataframe and sends it to the database (using the given engine)\n",
    "    \n",
    "    Takes:\n",
    "    - df: pandas Dataframe in correct format\n",
    "    - table: string of the table name\n",
    "    - engine: SQL-Alchemy engine\n",
    "    '''\n",
    "    df_name = get_df_name(df)\n",
    "    \n",
    "    #df.to_sql(table, engine, if_exists = 'append', index = False)\n",
    "    print(\"Inserted df '{df_name}' into table '{table_name}'.\".format(df_name = df_name, table_name = table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_swiss_snow_for_db(df):\n",
    "    '''\n",
    "    Transforms the downloaded snow data into the third normal form, ready to be stored in the database.\n",
    "    \n",
    "    Takes:\n",
    "    - Dataframe of swiss snow data\n",
    "    \n",
    "    Returns:\n",
    "    - 3 Dataframes (in this order!): \n",
    "        - res_locations\n",
    "        - res_sensors \n",
    "        - res_sensor_readings\n",
    "    '''\n",
    "    \n",
    "    df['Year'] = df['Year'].apply(str)\n",
    "    df['Year'] = pd.to_datetime(df['Year']).dt.date\n",
    "    res = df.copy()\n",
    "    \n",
    "    # transform for 'locations' table\n",
    "    loc_len = get_db_table_length('locations', credentials)\n",
    "    sensor_len = get_db_table_length('sensors', credentials)\n",
    "    \n",
    "    loc_name_rename = df['Region'].unique().tolist()\n",
    "    loc_id_rename = list(range(loc_len + 1, len(loc_name_rename) + loc_len + 1))\n",
    "    sensor_id = list(range(loc_len + 1, len(loc_name_rename) + loc_len + 1))\n",
    "    \n",
    "    res_location = res.drop(['Year', 'Neuschnee'], axis = 1).groupby(['Region'], as_index = False).last()\n",
    "    #res_location = pd.merge(res_location, countries, left_on='Country', right_on='db_name')\n",
    "    res_location = res_location.join(countries, on = 'Country')\n",
    "    res_location.rename(columns={'alpha3':'alpha_3'}, inplace=True)\n",
    "    #res_location['location_id'] = res_location.index + 1\n",
    "    res_location = res_location.drop(['url', 'db_name'], axis = 1)\n",
    "    res_location.columns = map(str.lower, res_location.columns)\n",
    "    \n",
    "    # transform for 'sensors' table\n",
    "    res_sensors = res # MAYBE FIX POINTER LATER? .copy()\n",
    "    res_sensors['location_id'] = res_sensors['Region'].replace(loc_name_rename, loc_id_rename)\n",
    "    res_sensors['sensor_type'] = 'new_snow'\n",
    "    res_sensors = res_sensors.drop(['Year','Country', 'Region', 'Neuschnee'], axis = 1).groupby(['location_id'], as_index = False).last()\n",
    "    res_sensors['sensor_id'] = sensor_id\n",
    "    res_sensors.columns = map(str.lower, res_sensors.columns)\n",
    "    \n",
    "    #transform for 'sensor_readings' table\n",
    "    res_sensor_readings = res.copy()\n",
    "    res_sensor_readings['int_reading'] = res['Neuschnee'].copy()\n",
    "    res_sensor_readings = pd.merge(res_sensor_readings, res_sensors, left_on='location_id', right_on='sensor_id')\n",
    "    res_sensor_readings = res_sensor_readings.drop([\n",
    "        'Country', 'Region', 'Neuschnee', 'sensor_type_x', 'sensor_type_y', 'location_id_x', 'location_id_y'\n",
    "    ], axis = 1)\n",
    "    res_sensor_readings.rename(columns={'Year':'timestamp'}, inplace=True)\n",
    "    res_sensor_readings.columns = map(str.lower, res_sensor_readings.columns)\n",
    "    \n",
    "    return res_location, res_sensors, res_sensor_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_swiss_sunshine_for_db(df):\n",
    "    '''\n",
    "    Transforms the downloaded sunshine data into the third normal form, ready to be stored in the database.\n",
    "    \n",
    "    Takes:\n",
    "    - Dataframe of swiss sunshine data\n",
    "    \n",
    "    Returns:\n",
    "    - 2 Dataframes (in this order!): \n",
    "        - res_sensors \n",
    "        - res_sensor_readings\n",
    "    '''\n",
    "    \n",
    "    df['Year'] = df['Year'].apply(str)\n",
    "    df['Year'] = pd.to_datetime(df['Year']).dt.date\n",
    "    res = df.copy()\n",
    "    \n",
    "    # transform for 'locations' table\n",
    "    loc_len = get_db_table_length('locations', credentials)\n",
    "    sensor_len = get_db_table_length('sensors', credentials)\n",
    "    \n",
    "    loc_name_rename = df['Region'].unique().tolist()\n",
    "    loc_id_rename = list(range(1, loc_len + 1))\n",
    "    sensor_id = list(range(loc_len + 1, len(loc_name_rename) + loc_len + 1))\n",
    "    \n",
    "    # transform for 'sensors' table\n",
    "    res_sensors = res\n",
    "    res_sensors['location_id'] = res_sensors['Region'].replace(loc_name_rename, loc_id_rename)\n",
    "    res_sensors['sensor_type'] = 'sunshine'\n",
    "    res_sensors = res_sensors.drop(['Year','Country', 'Region', 'Sonnenscheindauer'], axis = 1).groupby(['location_id'], as_index = False).last()\n",
    "    res_sensors['sensor_id'] = sensor_id\n",
    "    res_sensors.columns = map(str.lower, res_sensors.columns)\n",
    "    \n",
    "    #transform for 'sensor_readings' table\n",
    "    res_sensor_readings = res\n",
    "    res_sensor_readings['float_reading'] = res['Sonnenscheindauer'].copy()\n",
    "    res_sensor_readings['location_id'] = res_sensor_readings['Region'].replace(loc_name_rename, loc_id_rename)\n",
    "    res_sensor_readings = pd.merge(res_sensor_readings, res_sensors, left_on='location_id', right_on='location_id')\n",
    "    res_sensor_readings = res_sensor_readings.drop([\n",
    "        'Country', 'Region', 'Sonnenscheindauer', 'location_id', 'sensor_type_x', 'sensor_type_y'\n",
    "    ], axis = 1)\n",
    "    res_sensor_readings.rename(columns={'Year':'timestamp'}, inplace=True)\n",
    "    res_sensor_readings.columns = map(str.lower, res_sensor_readings.columns)\n",
    "    return res_sensors, res_sensor_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_swiss_precipitation_for_db(df):\n",
    "    '''\n",
    "    Transforms the downloaded precipitation data into the third normal form, ready to be stored in the database.\n",
    "    \n",
    "    Takes:\n",
    "    - Dataframe of swiss precipitation data\n",
    "    \n",
    "    Returns:\n",
    "    - 2 Dataframes (in this order!): \n",
    "        - res_sensors \n",
    "        - res_sensor_readings\n",
    "    '''\n",
    "    \n",
    "    df['Year'] = df['Year'].apply(str)\n",
    "    df['Year'] = pd.to_datetime(df['Year']).dt.date\n",
    "    res = df.copy()\n",
    "    \n",
    "    # transform for 'locations' table\n",
    "    loc_len = get_db_table_length('locations', credentials)\n",
    "    sensor_len = get_db_table_length('sensors', credentials)\n",
    "    \n",
    "    loc_name_rename = df['Region'].unique().tolist()\n",
    "    loc_id_rename = list(range(1, loc_len + 1))\n",
    "    sensor_id = list(range(sensor_len + 1, len(loc_name_rename) + sensor_len + 1))\n",
    "    \n",
    "    # transform for 'sensors' table\n",
    "    res_sensors = res.copy()\n",
    "    res_sensors['location_id'] = res_sensors['Region'].replace(loc_name_rename, loc_id_rename)\n",
    "    res_sensors['sensor_type'] = 'precipitation'\n",
    "    res_sensors = res_sensors.drop(['Year','Country', 'Region', 'Jahresniederschlag'], axis = 1).groupby(['location_id'], as_index = False).last()\n",
    "    res_sensors['sensor_id'] = sensor_id\n",
    "    res_sensors.columns = map(str.lower, res_sensors.columns)\n",
    "    \n",
    "    #transform for 'sensor_readings' table\n",
    "    res_sensor_readings = res\n",
    "    res_sensor_readings['float_reading'] = res['Jahresniederschlag'].copy()\n",
    "    res_sensor_readings['location_id'] = res_sensor_readings['Region'].replace(loc_name_rename, loc_id_rename)\n",
    "    res_sensor_readings = pd.merge(res_sensor_readings, res_sensors, left_on='location_id', right_on='location_id')\n",
    "    res_sensor_readings = res_sensor_readings.drop([\n",
    "        'Country', 'Region', 'Jahresniederschlag', 'location_id', 'sensor_type'\n",
    "    ], axis = 1)\n",
    "    res_sensor_readings.rename(columns={'Year':'timestamp'}, inplace=True)\n",
    "    res_sensor_readings.columns = map(str.lower, res_sensor_readings.columns)\n",
    "    \n",
    "    return res_sensors, res_sensor_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_swiss_temp_for_db(df):\n",
    "    '''\n",
    "    Transforms the downloaded precipitation data into the third normal form, ready to be stored in the database.\n",
    "    \n",
    "    Takes:\n",
    "    - Dataframe of swiss precipitation data\n",
    "    \n",
    "    Returns:\n",
    "    - 2 Dataframes (in this order!): \n",
    "        - res_sensors \n",
    "        - res_sensor_readings\n",
    "    '''\n",
    "    \n",
    "    df['Year'] = df['Year'].apply(str)\n",
    "    df['Year'] = pd.to_datetime(df['Year']).dt.date\n",
    "    res = df.copy()\n",
    "    \n",
    "    # transform for 'locations' table\n",
    "    loc_len = get_db_table_length('locations', credentials)\n",
    "    sensor_len = get_db_table_length('sensors', credentials)\n",
    "    \n",
    "    loc_name_rename = df['Region'].unique().tolist()\n",
    "    loc_id_rename = list(range(1, loc_len + 1))\n",
    "    sensor_id = list(range(sensor_len + 1, len(loc_name_rename) + sensor_len + 1))\n",
    "    \n",
    "    # transform for 'sensors' table\n",
    "    res_sensors = res.copy()\n",
    "    res_sensors['location_id'] = res_sensors['Region'].replace(loc_name_rename, loc_id_rename)\n",
    "    res_sensors['sensor_type'] = 'temperature'\n",
    "    res_sensors = res_sensors.drop(['Year','Country', 'Region', 'Jahrestemperatur'], axis = 1).groupby(['location_id'], as_index = False).last()\n",
    "    res_sensors['sensor_id'] = sensor_id\n",
    "    res_sensors.columns = map(str.lower, res_sensors.columns)\n",
    "    \n",
    "    #transform for 'sensor_readings' table\n",
    "    res_sensor_readings = res\n",
    "    res_sensor_readings['float_reading'] = res['Jahrestemperatur'].copy()\n",
    "    res_sensor_readings['location_id'] = res_sensor_readings['Region'].replace(loc_name_rename, loc_id_rename)\n",
    "    res_sensor_readings = pd.merge(res_sensor_readings, res_sensors, left_on='location_id', right_on='location_id')\n",
    "    res_sensor_readings = res_sensor_readings.drop([\n",
    "        'Country', 'Region', 'Jahrestemperatur', 'location_id', 'sensor_type'\n",
    "    ], axis = 1)\n",
    "    res_sensor_readings.rename(columns={'Year':'timestamp'}, inplace=True)\n",
    "    res_sensor_readings.columns = map(str.lower, res_sensor_readings.columns)\n",
    "    \n",
    "    return res_sensors, res_sensor_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_temperature_for_db(df):\n",
    "    '''\n",
    "    Transforms the downloaded temperature data into the third normal form, ready to be stored in the database.\n",
    "    \n",
    "    Takes:\n",
    "    - Dataframe of global temperature data\n",
    "    \n",
    "    Returns:\n",
    "    - 3 Dataframes (in this order!): \n",
    "        - res_locations\n",
    "        - res_sensors \n",
    "        - res_sensor_readings\n",
    "    '''\n",
    "    \n",
    "    res = df.copy()\n",
    "    \n",
    "    # transform for 'locations' table\n",
    "    loc_len = get_db_table_length('locations', credentials)\n",
    "    sensor_len = get_db_table_length('sensors', credentials)\n",
    "    reading_len = get_db_table_length('sensor_readings', credentials)\n",
    "    \n",
    "    temp_name_rename = df['country'].unique().tolist()\n",
    "    temp_id_rename = list(range(loc_len + 1, len(temp_name_rename) + loc_len + 1))\n",
    "    sensor_id = list(range(sensor_len + 1, len(temp_id_rename) + sensor_len + 1))\n",
    "    \n",
    "    res_location = res.groupby(['country'], as_index = False).last()\n",
    "    res_location['location_id'] = temp_id_rename\n",
    "    res_location = res_location.join(countries, on = 'country')\n",
    "    res_location.rename(columns={'date':'timestamp', 'alpha3':'alpha_3'}, inplace=True)\n",
    "    #res_location['location_id'] = res_location.index + 1\n",
    "    res_location = res_location.drop(['url', 'db_name', 'timestamp', 'monthly_anomaly'], axis = 1)\n",
    "    res_location.columns = map(str.lower, res_location.columns)\n",
    "    \n",
    "    # transform for 'sensors' table\n",
    "    res_sensors = res.copy()\n",
    "    res_sensors['location_id'] = res_sensors['country'].replace(temp_name_rename, temp_id_rename)\n",
    "    res_sensors['sensor_type'] = 'temperature'\n",
    "    res_sensors = res_sensors.drop(['date','country', 'monthly_anomaly'], axis = 1).groupby(['location_id'], as_index = False).last()\n",
    "    res_sensors['sensor_id'] = sensor_id\n",
    "    res_sensors.columns = map(str.lower, res_sensors.columns)\n",
    "    \n",
    "    #transform for 'sensor_readings' table\n",
    "    res_sensor_readings = res.copy()\n",
    "    res_sensor_readings['float_reading'] = res['monthly_anomaly'].copy().round(3)\n",
    "    r_sen_id = pd.merge(res_location,res_sensors, how = 'left')\n",
    "    res_sensor_readings = pd.merge(res_sensor_readings, r_sen_id, how='left', on='country')\n",
    "    res_sensor_readings.rename(columns={'date':'timestamp'}, inplace=True)\n",
    "    res_sensor_readings.columns = map(str.lower, res_sensor_readings.columns)\n",
    "    res_sensor_readings['int_reading'] = np.nan\n",
    "    res_sensor_readings['bool_reading'] = np.nan\n",
    "    res_sensor_readings['sensor_reading_id'] = res_sensor_readings.index + reading_len + 1\n",
    "    res_sensor_readings = res_sensor_readings[['sensor_reading_id', 'timestamp', 'int_reading', 'float_reading', 'bool_reading','sensor_id']]\n",
    "    return res_location, res_sensors, res_sensor_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_co2_for_db(df):\n",
    "    '''\n",
    "    Transforms the downloaded co2 data into the third normal form, ready to be stored in the database.\n",
    "    \n",
    "    Takes:\n",
    "    - Dataframe of global co2 data\n",
    "    \n",
    "    Returns:\n",
    "    - 2 Dataframes (in this order!):\n",
    "        - res_sensors \n",
    "        - res_sensor_readings\n",
    "    '''\n",
    "    \n",
    "    res = df.copy()\n",
    "    \n",
    "    co2_name_rename = df['country'].unique().tolist()\n",
    "    loc_len = get_db_table_length('locations', credentials)\n",
    "    co2_id_rename = list(range(loc_len + 1, len(co2_name_rename) + loc_len + 1))\n",
    "    co2_id_rename = [x - len(co2_name_rename) for x in co2_id_rename]\n",
    "    \n",
    "    # transform for 'locations' table\n",
    "    sensor_len = get_db_table_length('sensors', credentials)\n",
    "    sensor_id = list(range(sensor_len + 1, len(co2_id_rename) + sensor_len + 1))\n",
    "    reading_len = get_db_table_length('sensor_readings', credentials)\n",
    "    \n",
    "    loc_sql = pd.read_sql('locations', engine)[['location_id', 'country']]\n",
    "    loc_sql = loc_sql.groupby(['country'], as_index = False).max()\n",
    "    \n",
    "    # locations exist, all countries are in the 'temperature step' created\n",
    "    \n",
    "    # transform for 'sensors' table\n",
    "    res_sensors = res\n",
    "    res_sensors['location_id'] = res_sensors['country'].replace(loc_sql['country'].tolist(), loc_sql['location_id'].tolist())\n",
    "    res_sensors['sensor_type'] = 'co2'\n",
    "    res_sensors = res_sensors.drop(['year','country', 'co2'], axis = 1).groupby(['location_id'], as_index = False).last()\n",
    "    res_sensors['sensor_id'] = sensor_id\n",
    "    res_sensors.columns = map(str.lower, res_sensors.columns)\n",
    "    \n",
    "    #transform for 'sensor_readings' table\n",
    "    res_sensor_readings = res.copy()\n",
    "    res_sensor_readings['float_reading'] = res['co2'].copy().round(5)\n",
    "    res_sensor_readings = pd.merge(res_sensor_readings, res_sensors, how='left', left_on='location_id', right_on='location_id')\n",
    "    res_sensor_readings.rename(columns={'year':'timestamp'}, inplace=True)\n",
    "    res_sensor_readings.columns = map(str.lower, res_sensor_readings.columns)\n",
    "    res_sensor_readings['int_reading'] = np.nan\n",
    "    res_sensor_readings['bool_reading'] = np.nan\n",
    "    res_sensor_readings['sensor_reading_id'] = res_sensor_readings.index + reading_len + 1\n",
    "    res_sensor_readings = res_sensor_readings[['sensor_reading_id', 'timestamp', 'int_reading', 'float_reading', 'bool_reading','sensor_id']]\n",
    "    \n",
    "    return res_sensors, res_sensor_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'climate_change_db'\n",
    "db_user = 'db_user'\n",
    "db_user_pw = 'db_password'\n",
    "db_adress = '86.119.42.47'\n",
    "db_port = '5432'\n",
    "\n",
    "engine = create_engine('postgresql://{user}:{password}@{host}:{port}/{db}'.format(user = db_user, password = db_user_pw, \n",
    "                                                                                        host = db_adress, port = db_port, db = db_name), echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataframes and then immediately send them to the database\n",
    "The generation of the next location_ids depends on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swiss data\n",
    "### Snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_swiss_snow_loc, db_swiss_snow_sensors, db_swiss_snow_sensor_readings = transform_swiss_snow_for_db(df_swiss_snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_swiss_snow_loc.to_sql('locations', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_swiss_snow_sensors.to_sql('sensors', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_swiss_snow_sensor_readings.to_sql('sensor_readings', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sunshine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_swiss_sunshine_sensors, db_swiss_sunshine_sensor_readings = transform_swiss_sunshine_for_db(df_swiss_sunshine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_swiss_sunshine_sensors.to_sql('sensors', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_swiss_sunshine_sensor_readings.to_sql('sensor_readings', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_swiss_precipitation_sensors, db_swiss_precipitation_sensor_readings = transform_swiss_precipitation_for_db(df_swiss_precipitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_swiss_precipitation_sensors.to_sql('sensors', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_swiss_precipitation_sensor_readings.to_sql('sensor_readings', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_swiss_temp_sensors, db_swiss_temp_sensor_readings = transform_swiss_temp_for_db(df_swiss_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_swiss_temp_sensors.to_sql('sensors', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_swiss_temp_sensor_readings.to_sql('sensor_readings', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global data\n",
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_temp_location, db_temp_sensors, db_temp_sensor_readings = transform_temperature_for_db(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_temp_location.to_sql('locations', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_temp_sensors.to_sql('sensors', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes way too long, aborted after over 30 min (probably takes around 1.5 hours to run)\n",
    "\n",
    "#%time db_temp_sensor_readings.to_sql('sensor_readings', engine, if_exists = 'append', index = False)\n",
    "\n",
    "# read in as csv\n",
    "db_temp_sensor_readings.to_csv('db_temp_sensor_readings.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Read in csv via pgAdmin\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carbon dioxide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_co2_sensors, db_co2_sensor_readings = transform_co2_for_db(df_co2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_co2_sensors.to_sql('sensors', engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time db_co2_sensor_readings.to_sql('sensor_readings', engine, if_exists = 'append', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
